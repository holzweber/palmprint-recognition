{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:47.023941Z",
     "iopub.status.busy": "2022-04-29T09:11:47.023519Z",
     "iopub.status.idle": "2022-04-29T09:11:48.501361Z",
     "shell.execute_reply": "2022-04-29T09:11:48.500269Z",
     "shell.execute_reply.started": "2022-04-29T09:11:47.023858Z"
    },
    "papermill": {
     "duration": 1.554746,
     "end_time": "2022-04-21T06:43:14.943997",
     "exception": false,
     "start_time": "2022-04-21T06:43:13.389251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function, Variable\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "import copy\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:48.509267Z",
     "iopub.status.busy": "2022-04-29T09:11:48.508557Z",
     "iopub.status.idle": "2022-04-29T09:11:48.574793Z",
     "shell.execute_reply": "2022-04-29T09:11:48.573534Z",
     "shell.execute_reply.started": "2022-04-29T09:11:48.509176Z"
    },
    "papermill": {
     "duration": 0.128377,
     "end_time": "2022-04-21T06:43:15.122759",
     "exception": false,
     "start_time": "2022-04-21T06:43:14.994382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:48.580646Z",
     "iopub.status.busy": "2022-04-29T09:11:48.57975Z",
     "iopub.status.idle": "2022-04-29T09:11:52.270674Z",
     "shell.execute_reply": "2022-04-29T09:11:52.269644Z",
     "shell.execute_reply.started": "2022-04-29T09:11:48.580606Z"
    },
    "papermill": {
     "duration": 6.784299,
     "end_time": "2022-04-21T06:43:21.954406",
     "exception": false,
     "start_time": "2022-04-21T06:43:15.170107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: np.asarray(x).copy()),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: np.asarray(x).copy()),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "#data_dir = './drive/MyDrive/Tongji'\n",
    "data_dir = '../input/tjfull/TJ_Full'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=256,\n",
    "                                             shuffle=True, num_workers=1)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02661,
     "end_time": "2022-04-21T06:43:22.068571",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.041961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Network defninitions\n",
    "\n",
    "The method 'compute_partial_repr' and the class  'TPSGridGen' are reused from the github repository: https://github.com/WarBean/tps_stn_pytorch\n",
    "Contact Details: warbean@qq.com #thin-plate-splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.286122Z",
     "iopub.status.busy": "2022-04-29T09:11:52.285589Z",
     "iopub.status.idle": "2022-04-29T09:11:52.30921Z",
     "shell.execute_reply": "2022-04-29T09:11:52.308134Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.286074Z"
    },
    "papermill": {
     "duration": 0.046178,
     "end_time": "2022-04-21T06:43:22.141786",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.095608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# phi(x1, x2) = r^2 * log(r), where r = ||x1 - x2||_2\n",
    "def compute_partial_repr(input_points, control_points):\n",
    "    N = input_points.size(0)\n",
    "    M = control_points.size(0)\n",
    "    pairwise_diff = input_points.view(N, 1, 2) - control_points.view(1, M, 2)\n",
    "    # original implementation, very slow\n",
    "    # pairwise_dist = torch.sum(pairwise_diff ** 2, dim = 2) # square of distance\n",
    "    pairwise_diff_square = pairwise_diff * pairwise_diff\n",
    "    pairwise_dist = pairwise_diff_square[:, :, 0] + pairwise_diff_square[:, :, 1]\n",
    "    repr_matrix = 0.5 * pairwise_dist * torch.log(pairwise_dist)\n",
    "    # fix numerical error for 0 * log(0), substitute all nan with 0\n",
    "    mask = repr_matrix != repr_matrix\n",
    "    repr_matrix.masked_fill_(mask, 0)\n",
    "    return repr_matrix\n",
    "\n",
    "class TPSGridGen(nn.Module):\n",
    "\n",
    "    def __init__(self, target_height, target_width, target_control_points):\n",
    "        super(TPSGridGen, self).__init__()\n",
    "        assert target_control_points.ndimension() == 2\n",
    "        assert target_control_points.size(1) == 2\n",
    "        N = target_control_points.size(0)\n",
    "        self.num_points = N\n",
    "        target_control_points = target_control_points.float()\n",
    "        # create padded kernel matrix\n",
    "        forward_kernel = torch.zeros(N + 3, N + 3)\n",
    "        target_control_partial_repr = compute_partial_repr(target_control_points, target_control_points)\n",
    "        forward_kernel[:N, :N].copy_(target_control_partial_repr)\n",
    "        forward_kernel[:N, -3].fill_(1)\n",
    "        forward_kernel[-3, :N].fill_(1)\n",
    "        forward_kernel[:N, -2:].copy_(target_control_points)\n",
    "        forward_kernel[-2:, :N].copy_(target_control_points.transpose(0, 1))\n",
    "        # compute inverse matrix\n",
    "        inverse_kernel = torch.inverse(forward_kernel)\n",
    "\n",
    "        # create target cordinate matrix\n",
    "        HW = target_height * target_width\n",
    "        target_coordinate = list(itertools.product(range(target_height), range(target_width)))\n",
    "        target_coordinate = torch.Tensor(target_coordinate) # HW x 2\n",
    "        Y, X = target_coordinate.split(1, dim = 1)\n",
    "        Y = Y * 2 / (target_height - 1) - 1\n",
    "        X = X * 2 / (target_width - 1) - 1\n",
    "        target_coordinate = torch.cat([X, Y], dim = 1) # convert from (y, x) to (x, y)\n",
    "        target_coordinate_partial_repr = compute_partial_repr(target_coordinate, target_control_points)\n",
    "        target_coordinate_repr = torch.cat([\n",
    "            target_coordinate_partial_repr, torch.ones(HW, 1), target_coordinate\n",
    "        ], dim = 1)\n",
    "\n",
    "        # register precomputed matrices\n",
    "        self.register_buffer('inverse_kernel', inverse_kernel)\n",
    "        self.register_buffer('padding_matrix', torch.zeros(3, 2))\n",
    "        self.register_buffer('target_coordinate_repr', target_coordinate_repr)\n",
    "\n",
    "    def forward(self, source_control_points):\n",
    "        assert source_control_points.ndimension() == 3\n",
    "        assert source_control_points.size(1) == self.num_points\n",
    "        assert source_control_points.size(2) == 2\n",
    "        batch_size = source_control_points.size(0)\n",
    "\n",
    "        Y = torch.cat([source_control_points, Variable(self.padding_matrix.expand(batch_size, 3, 2))], 1)\n",
    "        mapping_matrix = torch.matmul(Variable(self.inverse_kernel), Y)\n",
    "        source_coordinate = torch.matmul(Variable(self.target_coordinate_repr), mapping_matrix)\n",
    "        return source_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.312401Z",
     "iopub.status.busy": "2022-04-29T09:11:52.311567Z",
     "iopub.status.idle": "2022-04-29T09:11:52.32696Z",
     "shell.execute_reply": "2022-04-29T09:11:52.325913Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.312352Z"
    },
    "papermill": {
     "duration": 0.037921,
     "end_time": "2022-04-21T06:43:22.20819",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.170269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ROILAnet(nn.Module):\n",
    "    def __init__(self, h=56, w=56, L=18):\n",
    "        super(ROILAnet, self).__init__()\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.L = L\n",
    "        vgg16 = models.vgg16(pretrained=True) # load vgg16 with pretrained weights\n",
    "        vgg16 = vgg16.features # only get feature block\n",
    "        vgg16 = vgg16[0:18] # cut off after first three conv-blocks\n",
    "        vgg16[-1] = torch.nn.LocalResponseNorm(512*2, 1e-6, 1, 0.5) #local response normalisationÂ´\n",
    "        self.featureExtractionCNN = vgg16\n",
    "        self.featureExtractionCNN.requires_grads=False\n",
    "        # Regression network\n",
    "        self.regressionNet = nn.Sequential(\n",
    "            nn.Linear(int(self.h/8) * int(self.w/8) * 256, 512),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, self.L)\n",
    "        )      \n",
    "    \n",
    "    def forward(self,I_resized):\n",
    "        # Pass to feature extraction CNN\n",
    "        feat = self.featureExtractionCNN(I_resized)\n",
    "        feat  = feat.view(-1, int(self.h/8) * int(self.w/8) * 256)\n",
    "        # Pass to regression network\n",
    "        theta = self.regressionNet(feat)\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027372,
     "end_time": "2022-04-21T06:43:22.262798",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.235426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defnition of Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.329577Z",
     "iopub.status.busy": "2022-04-29T09:11:52.328794Z",
     "iopub.status.idle": "2022-04-29T09:11:52.343026Z",
     "shell.execute_reply": "2022-04-29T09:11:52.341903Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.329533Z"
    },
    "papermill": {
     "duration": 0.034898,
     "end_time": "2022-04-21T06:43:22.324724",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.289826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadROIModel(weightPath: str = None):\n",
    "    \"\"\"\n",
    "    @weightPath: path to the ROILAnet() weights\n",
    "    loads localization network with pretrained weights\n",
    "    \"\"\"\n",
    "    model = ROILAnet()\n",
    "    model.load_state_dict(torch.load(weightPath, map_location=torch.device('cpu')))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    model.requires_grads=False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.34571Z",
     "iopub.status.busy": "2022-04-29T09:11:52.34497Z",
     "iopub.status.idle": "2022-04-29T09:11:52.354286Z",
     "shell.execute_reply": "2022-04-29T09:11:52.353398Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.345619Z"
    },
    "papermill": {
     "duration": 0.034694,
     "end_time": "2022-04-21T06:43:22.386443",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.351749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getThinPlateSpline(target_width: int = 112, target_height: int = 112) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    @target_width: desired I_ROI output width\n",
    "    @target_height: desired I_ROI output height\n",
    "    greates instance of TPS grid generator\n",
    "    \"\"\"\n",
    "    # creat control points\n",
    "    target_control_points = torch.Tensor(list(itertools.product(\n",
    "        torch.arange(-1.0, 1.00001, 1.0),\n",
    "        torch.arange(-1.0, 1.00001, 1.0),\n",
    "    )))\n",
    "    gridgen = TPSGridGen(target_height=target_height, target_width=target_width, target_control_points=target_control_points)\n",
    "    gridgen = gridgen.to(device)\n",
    "    return gridgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.35704Z",
     "iopub.status.busy": "2022-04-29T09:11:52.356289Z",
     "iopub.status.idle": "2022-04-29T09:11:52.370132Z",
     "shell.execute_reply": "2022-04-29T09:11:52.369086Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.356991Z"
    },
    "papermill": {
     "duration": 0.036815,
     "end_time": "2022-04-21T06:43:22.450105",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.41329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getOriginalAndResizedInput(path: str = None) -> (np.ndarray, torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    @path: image which should be loaded from database\n",
    "    This function load the image of variable size from a directory given in path.\n",
    "    After doing the resizing to 56x56 pixels, the original and resized image will be returned\n",
    "    as (PILMain, source_image, resizedImage) triplet\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        return (None, None)\n",
    "    \n",
    "    #define transformer for resized input of feature extraction CNN\n",
    "    resizeTranformer = transforms.Compose([\n",
    "            transforms.Resize((56,56)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    PILMain = Image.open(path).convert(mode = 'RGB') # load image in PIL format\n",
    "    sourceImage = np.array(PILMain).astype('float64') # convert from PIL to float64\n",
    "    sourceImage = transforms.ToTensor()(sourceImage).unsqueeze_(0) # add first dimension, which is batch dim\n",
    "    sourceImage = sourceImage.to(device) # load to available device\n",
    "\n",
    "    resizedImage = resizeTranformer(PILMain)\n",
    "    resizedImage = resizedImage.view(-1,resizedImage.size(0),resizedImage.size(1),resizedImage.size(2))\n",
    "    resizedImage = resizedImage.to(device) # load to available device\n",
    "    return (PILMain, sourceImage,resizedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.374059Z",
     "iopub.status.busy": "2022-04-29T09:11:52.372846Z",
     "iopub.status.idle": "2022-04-29T09:11:52.383331Z",
     "shell.execute_reply": "2022-04-29T09:11:52.382287Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.374008Z"
    },
    "papermill": {
     "duration": 0.03473,
     "end_time": "2022-04-21T06:43:22.511801",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.477071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getThetaHat(resizedImage: torch.Tensor = None, model = None) -> torch.Tensor: \n",
    "    \"\"\"\n",
    "    @resizedImage: cropped image\n",
    "    @model: ROI Localisation network, which outputs a theta vector\n",
    "    resizedImage: image which should is loaded from database via getOriginalAndResizedInput function\n",
    "    Here the theta vector is calculated using the pretrained localisation network. The vector has a size of\n",
    "    [9, 2] -> which stand for 9 pairs of x and y values\n",
    "    \"\"\"\n",
    "    if resizedImage is None:\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad(): # deactivate gradients because we try to predict the ROI\n",
    "        theta_hat = model.forward(resizedImage)\n",
    "    theta_hat = theta_hat.view(-1, 2, 9) # split into x and y vector -> theta_hat is originally a vector like [xxxxxxxxxyyyyyyyyy]\n",
    "    theta_hat = torch.stack((theta_hat[:,0], theta_hat[:,1]),-1)\n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.386076Z",
     "iopub.status.busy": "2022-04-29T09:11:52.385355Z",
     "iopub.status.idle": "2022-04-29T09:11:52.398571Z",
     "shell.execute_reply": "2022-04-29T09:11:52.397599Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.385931Z"
    },
    "papermill": {
     "duration": 0.034411,
     "end_time": "2022-04-21T06:43:22.57317",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.538759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampleGrid(theta_hat: torch.Tensor = None, sourceImage: torch.Tensor = None, target_width: int = 112, target_height: int = 112 ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    @theta_hat: theta vector of normlized x,y coordinate pairs\n",
    "    @sourceImage: the original image without any crops or resizsing\n",
    "    @target_width: output IROI target width\n",
    "    @target_height: output IROI target height\n",
    "    Samples grid from a given theta vector, source image and grid generator\n",
    "    \"\"\"\n",
    "    gridgen = getThinPlateSpline(target_width, target_height)\n",
    "    #generate grid from calculated theta_hat vector\n",
    "    source_coordinate = gridgen(theta_hat)\n",
    "    #create target grid - with target height and target width\n",
    "    grid = source_coordinate.view(-1, target_height, target_width, 2).to(device)\n",
    "    #sample ROI from input image and created T(theta_hat)\n",
    "    target_image = F.grid_sample(sourceImage, grid)\n",
    "    return target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.40301Z",
     "iopub.status.busy": "2022-04-29T09:11:52.402343Z",
     "iopub.status.idle": "2022-04-29T09:11:52.412254Z",
     "shell.execute_reply": "2022-04-29T09:11:52.41125Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.402975Z"
    },
    "papermill": {
     "duration": 0.033579,
     "end_time": "2022-04-21T06:43:22.63356",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.599981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def printExtraction(target_image: torch.Tensor = None, source_image = None):\n",
    "    \"\"\"\n",
    "    @source_image: prints the source_image which is in the PIL format\n",
    "    @target_image: print the target_image which is a tensor (ROI)\n",
    "    \"\"\"\n",
    "    #prepare to show -> get back from gpu if needed\n",
    "    target_image = target_image.cpu().data.numpy().squeeze().swapaxes(0, 1).swapaxes(1, 2)\n",
    "    target_image = Image.fromarray(target_image.astype('uint8'))\n",
    "    plt.imshow(source_image)\n",
    "    plt.show() # show original image\n",
    "    plt.imshow(target_image)\n",
    "    plt.show() # show ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.418972Z",
     "iopub.status.busy": "2022-04-29T09:11:52.417988Z",
     "iopub.status.idle": "2022-04-29T09:11:52.426248Z",
     "shell.execute_reply": "2022-04-29T09:11:52.425075Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.418882Z"
    },
    "papermill": {
     "duration": 0.033605,
     "end_time": "2022-04-21T06:43:22.694147",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.660542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadCNNModel(weightPath: str = None):\n",
    "    \"\"\"\n",
    "    @weightPath: path to the ROILAnet() weights\n",
    "    loads localization network with pretrained weights\n",
    "    \"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "    model.load_state_dict(torch.load(weightPath))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.428894Z",
     "iopub.status.busy": "2022-04-29T09:11:52.42784Z",
     "iopub.status.idle": "2022-04-29T09:11:52.43697Z",
     "shell.execute_reply": "2022-04-29T09:11:52.435742Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.42882Z"
    },
    "papermill": {
     "duration": 0.036949,
     "end_time": "2022-04-21T06:43:22.762384",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.725435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getIROI(model, input):\n",
    "    resizedImage = F.interpolate(input, (56, 56))\n",
    "    theta_hat = getThetaHat(resizedImage=resizedImage, model=model) # create theta hat with normlized ROI coordinates\n",
    "    IROI = sampleGrid(theta_hat=theta_hat, sourceImage=input, target_width=224, target_height=224) # get ROI from source image\n",
    "    IROI.to(device)\n",
    "    return IROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.439788Z",
     "iopub.status.busy": "2022-04-29T09:11:52.439124Z",
     "iopub.status.idle": "2022-04-29T09:11:52.451526Z",
     "shell.execute_reply": "2022-04-29T09:11:52.450304Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.439738Z"
    },
    "papermill": {
     "duration": 0.037565,
     "end_time": "2022-04-21T06:43:22.827184",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.789619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getOriginalAndResizedInput(PILMain) -> (np.ndarray, torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    @path: image which should be loaded from database\n",
    "    This function load the image of variable size from a directory given in path.\n",
    "    After doing the resizing to 56x56 pixels, the original and resized image will be returned\n",
    "    as (PILMain, source_image, resizedImage) triplet\n",
    "    \"\"\"\n",
    "    if PILMain is None:\n",
    "        return (None, None)\n",
    "    \n",
    "    #define transformer for resized input of feature extraction CNN\n",
    "    resizeTranformer = transforms.Compose([\n",
    "            transforms.Resize((56,56)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    #PILMain = PILMain.convert(mode = 'RGB') # load image in PIL format\n",
    "    sourceImage = np.array(PILMain).astype('float64') # convert from PIL to float64\n",
    "    sourceImage = transforms.ToTensor()(sourceImage).unsqueeze_(0) # add first dimension, which is batch dim\n",
    "    sourceImage = sourceImage.to(device) # load to available device\n",
    "\n",
    "    resizedImage = resizeTranformer(PILMain)\n",
    "    resizedImage = resizedImage.view(-1,resizedImage.size(0),resizedImage.size(1),resizedImage.size(2))\n",
    "    resizedImage = resizedImage.to(device) # load to available device\n",
    "    return (PILMain, sourceImage,resizedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:11:52.453929Z",
     "iopub.status.busy": "2022-04-29T09:11:52.45352Z",
     "iopub.status.idle": "2022-04-29T09:12:02.243195Z",
     "shell.execute_reply": "2022-04-29T09:12:02.242175Z",
     "shell.execute_reply.started": "2022-04-29T09:11:52.453847Z"
    },
    "papermill": {
     "duration": 23.318982,
     "end_time": "2022-04-21T06:43:46.175407",
     "exception": false,
     "start_time": "2022-04-21T06:43:22.856425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROIModelPath = '../input/roilanet/ROI_extractor_augmented_TJ-NTU.pt' # path to pretrained network weights\n",
    "CNNModelPath = '../input/restnet18/resnet18_tongji_unfreezed.pt'\n",
    "\n",
    "#load localisation netowork\n",
    "localisationNetwork = loadROIModel(ROIModelPath) # load localisation network\n",
    "\n",
    "#recognition Network setup pretrained\n",
    "recognitionNetwork = loadCNNModel(CNNModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:12:02.245712Z",
     "iopub.status.busy": "2022-04-29T09:12:02.245079Z",
     "iopub.status.idle": "2022-04-29T09:12:02.253585Z",
     "shell.execute_reply": "2022-04-29T09:12:02.252563Z",
     "shell.execute_reply.started": "2022-04-29T09:12:02.245648Z"
    },
    "papermill": {
     "duration": 0.038789,
     "end_time": "2022-04-21T06:43:46.246557",
     "exception": false,
     "start_time": "2022-04-21T06:43:46.207768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.Adam(recognitionNetwork.parameters(), lr=0.0005)\n",
    "# Decay LR by a factor of 0.1 every 25 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:12:02.255905Z",
     "iopub.status.busy": "2022-04-29T09:12:02.255516Z",
     "iopub.status.idle": "2022-04-29T09:12:02.283126Z",
     "shell.execute_reply": "2022-04-29T09:12:02.282045Z",
     "shell.execute_reply.started": "2022-04-29T09:12:02.25583Z"
    },
    "papermill": {
     "duration": 0.050165,
     "end_time": "2022-04-21T06:43:46.326732",
     "exception": false,
     "start_time": "2022-04-21T06:43:46.276567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(localisation, model, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    localisation.eval()\n",
    "    grayTransformer = transforms.Compose([\n",
    "                    transforms.CenterCrop((224,224)),\n",
    "                    transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_lss, val_lss, train_acc,val_acc = [], [], [], []\n",
    "    since = time.time() #starting time\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for numpy_image, labels in dataloaders[phase]:\n",
    "                labels = labels.to(device)\n",
    "                with torch.no_grad(): # deactivate gradients because we try to predict the ROI\n",
    "                    target_batch = []\n",
    "                    source_img_batch = []\n",
    "                    #NEW VERSION --------\n",
    "                    for idx, b in enumerate(numpy_image):\n",
    "                        inputPIL = Image.fromarray(np.uint8(b)).convert('RGB')\n",
    "                        (PILMain, sourceImage,resizedImage) = getOriginalAndResizedInput(inputPIL)\n",
    "                        sourceImage = sourceImage.squeeze()\n",
    "                        resizedImage = resizedImage.squeeze()\n",
    "                        target_batch.append(resizedImage)\n",
    "                        source_img_batch.append(sourceImage)\n",
    "                    target_batch = torch.stack(target_batch)\n",
    "                    source_img_batch = torch.stack(source_img_batch)\n",
    "                    #get normalized coordinates\n",
    "                    theta_hat = getThetaHat(target_batch, localisationNetwork)\n",
    "                    #get all ROIs\n",
    "                    IROI = sampleGrid(theta_hat=theta_hat, sourceImage=source_img_batch, target_width=300, target_height=300)\n",
    "                    source_img_batch = []\n",
    "                    target_batch = []\n",
    "                    for b in IROI:\n",
    "                        b = Image.fromarray(np.uint8(b.cpu()[0])).convert('L')\n",
    "                        target_batch.append(grayTransformer(b))\n",
    "                    target_batch = torch.stack(target_batch)\n",
    "                    target_batch = target_batch.to(device)\n",
    "                    #NEW VERSION END--------\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(target_batch)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * numpy_image.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            #add to lists\n",
    "            if phase == 'train':\n",
    "                train_lss.append(float(epoch_loss))\n",
    "                train_acc.append(float(epoch_acc))\n",
    "            else:\n",
    "                val_lss.append(float(epoch_loss))\n",
    "                val_acc.append(float(epoch_acc))\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_lss, val_lss, train_acc,val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T09:12:02.285657Z",
     "iopub.status.busy": "2022-04-29T09:12:02.284998Z",
     "iopub.status.idle": "2022-04-29T09:43:01.284331Z",
     "shell.execute_reply": "2022-04-29T09:43:01.280753Z",
     "shell.execute_reply.started": "2022-04-29T09:12:02.285608Z"
    },
    "papermill": {
     "duration": 31483.08103,
     "end_time": "2022-04-21T15:28:29.436716",
     "exception": false,
     "start_time": "2022-04-21T06:43:46.355686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft, train_lss, val_lss, train_acc,val_acc = train_model(localisationNetwork, recognitionNetwork, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-29T09:43:01.287514Z",
     "iopub.status.idle": "2022-04-29T09:43:01.288856Z",
     "shell.execute_reply": "2022-04-29T09:43:01.288507Z",
     "shell.execute_reply.started": "2022-04-29T09:43:01.288472Z"
    },
    "papermill": {
     "duration": 0.243325,
     "end_time": "2022-04-21T15:28:29.74082",
     "exception": false,
     "start_time": "2022-04-21T15:28:29.497495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'recognition_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-29T09:43:01.291515Z",
     "iopub.status.idle": "2022-04-29T09:43:01.292454Z",
     "shell.execute_reply": "2022-04-29T09:43:01.292145Z",
     "shell.execute_reply.started": "2022-04-29T09:43:01.292113Z"
    },
    "papermill": {
     "duration": 0.373224,
     "end_time": "2022-04-21T15:28:30.175225",
     "exception": false,
     "start_time": "2022-04-21T15:28:29.802001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochsLst = range(0,epochs)\n",
    "plt.plot(epochsLst, train_lss, 'g', label='Training loss')\n",
    "plt.plot(epochsLst, val_lss, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-29T09:43:01.294271Z",
     "iopub.status.idle": "2022-04-29T09:43:01.295069Z",
     "shell.execute_reply": "2022-04-29T09:43:01.294782Z",
     "shell.execute_reply.started": "2022-04-29T09:43:01.294752Z"
    },
    "papermill": {
     "duration": 0.414063,
     "end_time": "2022-04-21T15:28:30.650426",
     "exception": false,
     "start_time": "2022-04-21T15:28:30.236363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochsLst = range(0,epochs)\n",
    "plt.plot(epochsLst, train_acc, 'g', label='Training acc.')\n",
    "plt.plot(epochsLst, val_acc, 'b', label='validation acc.')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('accuracy.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
